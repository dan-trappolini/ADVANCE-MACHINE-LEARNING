{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [50]\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
    "                                           train=True,\n",
    "                                           transform=norm_transform,\n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
    "                                          train=False,\n",
    "                                          transform=norm_transform\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------------------------------\n",
    "# Data loader\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#======================================================================================\n",
    "# Q4: Implementing multi-layer perceptron in PyTorch\n",
    "#======================================================================================\n",
    "\n",
    "So far we have implemented a two-layer network using numpy by explicitly writing down the forward computation and deriving and implementing the equations for backward computation. This process can be tedious to extend to large network architectures\n",
    "\n",
    "Popular deep-learning libraries like PyTorch and Tensorflow allow us to quickly implement complicated neural network architectures. They provide pre-defined layers which can be used as building blocks to define our network. They also enable automatic-differentiation, which allows us to define only the forward pass and let the libraries perform back-propagation using automatic differentiation.\n",
    "\n",
    "In this question we will implement a multi-layer perceptron using the PyTorch library.  Please complete the code for the MultiLayerPerceptron, training and evaluating the model. Once you can train the two layer model, experiment with adding more layers and report your observations\n",
    "#--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#-------------------------------------------------\n",
    "# Fully connected neural network with one hidden layer\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        #################################################################################\n",
    "        # TODO: Initialize the modules required to implement the mlp with the layer     #\n",
    "        # configuration. input_size --> hidden_layers[0] --> hidden_layers[1] .... -->  #\n",
    "        # hidden_layers[-1] --> num_classes                                             #\n",
    "        # Make use of linear and relu layers from the torch.nn module                   #\n",
    "        #################################################################################\n",
    "        \n",
    "        layers = [] #Use the layers list to store a variable number of layers\n",
    "        \n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        if len(hidden_layers) == 0:\n",
    "            raise ValueError(\"ERROR:  Check the number of layers\")\n",
    "        \n",
    "        elif len(hidden_layers) == 1:\n",
    "            layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_layers[-1], num_classes))\n",
    "        \n",
    "        else:\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif i +1 == len(hidden_layers):\n",
    "                    layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    layers.append(nn.Linear(hidden_layers[i], num_classes))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                    layers.append(nn.ReLU())\n",
    "\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Enter the layers into nn.Sequential, so the model may \"see\" them\n",
    "        # Note the use of * in front of layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # TODO: Implement the forward pass computations                                 #\n",
    "        # Note that you do not need to use the softmax operation at the end.            #\n",
    "        # Softmax is only required for the loss computation and the criterion used below#\n",
    "        # nn.CrossEntropyLoss() already integrates the softmax and the log loss together#\n",
    "        #################################################################################\n",
    "        \n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        out = self.layers(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Model\\'s state_dict:\")\\nfor param_tensor in model.state_dict():\\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hidden_size = [50]\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes).to(device)\n",
    "# Print model's state_dict\n",
    "'''\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 51.3 %\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [50]\n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "\n",
    "if train:\n",
    "    model.apply(weights_init)\n",
    "    model.train() #set dropout and batch normalization layers to training mode\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "    # Train the model\n",
    "    lr = learning_rate\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #################################################################################\n",
    "            # TODO: Implement the training code                                             #\n",
    "            # 1. Pass the images to the model                                               #\n",
    "            # 2. Compute the loss using the output and the labels.                          #\n",
    "            # 3. Compute gradients and update the model using the optimizer                 #\n",
    "            # Use examples in https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "            #################################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            \n",
    "            # load images\n",
    "            images = images.view(images.size(0), -1)\n",
    "            predicted_lables = model(images)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = criterion(predicted_lables, labels)\n",
    "            \n",
    "            # Compute gradients and update the model using the optimizer\n",
    "            optimizer.zero_grad() # Prevent gradients from accumulating\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        # Code to update the lr\n",
    "        lr *= learning_rate_decay\n",
    "        update_lr(optimizer, lr)\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                ####################################################\n",
    "                # TODO: Implement the evaluation code              #\n",
    "                # 1. Pass the images to the model                  #\n",
    "                # 2. Get the most confident predicted class        #\n",
    "                ####################################################\n",
    "                # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "                # Pass the images to the model       \n",
    "                images = images.view(images.size(0), -1)\n",
    "                \n",
    "                # Get the most confident predicted class \n",
    "                predicted = torch.argmax(model(images), dim=1)\n",
    "                # y_pred = model(images)\n",
    "                # predicted = torch.max(y_pred, 1).indices\n",
    "\n",
    "                # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
    "            \n",
    "    ##################################################################################\n",
    "    # TODO: Now that you can train a simple two-layer MLP using above code, you can  #\n",
    "    # easily experiment with adding more layers and different layer configurations   #\n",
    "    # and let the pytorch library handle computing the gradients                     #\n",
    "    #                                                                                #\n",
    "    # Experiment with different number of layers (at least from 2 to 5 layers) and   #\n",
    "    # record the final validation accuracies Report your observations on how adding  #\n",
    "    # more layers to the MLP affects its behavior. Try to improve the model          #\n",
    "    # configuration using the validation performance as the guidance. You can        #\n",
    "    # experiment with different activation layers available in torch.nn, adding      #\n",
    "    # dropout layers, if you are interested. Use the best model on the validation    #\n",
    "    # set, to evaluate the performance on the test set once and report it            #\n",
    "    ##################################################################################\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    torch.save(model.state_dict(), 'model.ckpt')\n",
    "    \n",
    "else:\n",
    "    # Run the test code once you have your by setting train flag to false\n",
    "    # and loading the best model\n",
    "\n",
    "    best_model = None\n",
    "    best_model = torch.load('model.ckpt')\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval() #set dropout and batch normalization layers to evaluation mode\n",
    "    \n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ####################################################\n",
    "            # TODO: Implement the evaluation code              #\n",
    "            # 1. Pass the images to the model                  #\n",
    "            # 2. Get the most confident predicted class        #\n",
    "            ####################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            # Pass the images to the model       \n",
    "            images = images.view(images.size(0), -1)\n",
    "\n",
    "            # Get the most confident predicted class \n",
    "            predicted = torch.argmax(model(images), dim=1)\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if total == 1000:\n",
    "                break\n",
    "\n",
    "        print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'6lCWP1lh'.LO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
